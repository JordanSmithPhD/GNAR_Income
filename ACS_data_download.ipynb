{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanSmithPhD/GNAR_Income/blob/main/ACS_data_download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Connect to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esN3L5c61iJp",
        "outputId": "c6100399-3076-4a57-cee8-fc824dc056ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Download data from the American Community Survey API (2010-2016).\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "import requests\n",
        "from google.colab import files\n",
        "\n",
        "# Define the base URL structure\n",
        "base_url = \"https://api.census.gov/data/{year}/acs/{type}?get={fields}&for=place:*&in=state:*&key=aa6a45f29d73cec19281fde4a0d9b815fd751298\"\n",
        "\n",
        "# Define years, data types, and fields\n",
        "years = range(2010, 2017)  # 2010 to 2016\n",
        "types_and_fields = {\n",
        "    \"acs5/profile\": \"DP05_0001E,DP05_0001M,DP02_0082PE,DP02_0082PM,DP03_0043PE,DP03_0043PM,NAME\",\n",
        "    \"acs5\": \"B19083_001E,B19083_001M,B25004_006E,B25004_006M,B25001_001E,B25001_001M,NAME\",\n",
        "    \"acs5/subject\": \"S1901_C01_012E,S1901_C01_012M,S1501_C01_015E,S1501_C01_015M,S0101_C01_030E,S0101_C01_030M,NAME\",\n",
        "}\n",
        "\n",
        "# Generate URLs dynamically\n",
        "urls = []\n",
        "for year in years:\n",
        "    for data_type, fields in types_and_fields.items():\n",
        "        urls.append((base_url.format(year=year, type=data_type, fields=fields), year))  # Tuple of URL and year\n",
        "\n",
        "# Function to download and save data\n",
        "def download_and_save_data(url, year, output_filename):\n",
        "    try:\n",
        "        # Fetch the content from the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "\n",
        "        # Parse the content into a pandas DataFrame\n",
        "        df = pd.read_csv(io.StringIO(response.text))  # Assuming it's in CSV format\n",
        "\n",
        "        # Add the \"year\" column\n",
        "        df['year'] = year\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        df.to_csv(output_filename, index=False)\n",
        "\n",
        "        print(f\"Saved {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "# Loop through URLs and save each dataset\n",
        "for i, (url, year) in enumerate(urls, start=1):\n",
        "    download_and_save_data(url, year, f\"dataset{i}.csv\")"
      ],
      "metadata": {
        "id": "HDC8mgGx7d-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e86e75-4e65-4070-d0b6-1dbd60cfd454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved dataset1.csv\n",
            "Saved dataset2.csv\n",
            "Saved dataset3.csv\n",
            "Saved dataset4.csv\n",
            "Saved dataset5.csv\n",
            "Saved dataset6.csv\n",
            "Saved dataset7.csv\n",
            "Saved dataset8.csv\n",
            "Saved dataset9.csv\n",
            "Saved dataset10.csv\n",
            "Saved dataset11.csv\n",
            "Saved dataset12.csv\n",
            "Saved dataset13.csv\n",
            "Saved dataset14.csv\n",
            "Saved dataset15.csv\n",
            "Saved dataset16.csv\n",
            "Saved dataset17.csv\n",
            "Saved dataset18.csv\n",
            "Saved dataset19.csv\n",
            "Saved dataset20.csv\n",
            "Saved dataset21.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Download data from the American Community Survey API (2017-2023).\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "import requests\n",
        "from google.colab import files\n",
        "\n",
        "# Define the base URL structure\n",
        "base_url = \"https://api.census.gov/data/{year}/acs/{type}?get={fields}&for=place:*&in=state:*&key=[YOUR API KEY HERE]\"\n",
        "\n",
        "# Define years, data types, and fields\n",
        "years = range(2017, 2024)  # 2017 to 2018\n",
        "types_and_fields = {\n",
        "    \"acs5/profile\": \"DP05_0001E,DP05_0001M,DP02_0082PE,DP02_0082PM,DP03_0043PE,DP03_0043PM,NAME\",\n",
        "    \"acs5\": \"B19083_001E,B19083_001M,B25004_006E,B25004_006M,B25001_001E,B25001_001M,NAME\",\n",
        "    \"acs5/subject\": \"S1901_C01_012E,S1901_C01_012M,S1501_C02_015E,S1501_C02_015M,S0101_C01_032E,S0101_C01_032M,NAME\",\n",
        "}\n",
        "\n",
        "# Generate URLs dynamically\n",
        "urls = []\n",
        "for year in years:\n",
        "    for data_type, fields in types_and_fields.items():\n",
        "        urls.append((base_url.format(year=year, type=data_type, fields=fields), year))  # Tuple of URL and year\n",
        "\n",
        "# Function to download and save data\n",
        "def download_and_save_data(url, year, output_filename):\n",
        "    try:\n",
        "        # Fetch the content from the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure the request was successful\n",
        "\n",
        "        # Parse the content into a pandas DataFrame\n",
        "        df = pd.read_csv(io.StringIO(response.text))  # Assuming it's in CSV format\n",
        "\n",
        "        # Add the \"year\" column\n",
        "        df['year'] = year\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        df.to_csv(output_filename, index=False)\n",
        "\n",
        "        print(f\"Saved {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {e}\")\n",
        "\n",
        "# Loop through URLs and save each dataset\n",
        "for i, (url, year) in enumerate(urls, start=22):\n",
        "    download_and_save_data(url, year, f\"dataset{i}.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4dt3zHoHFc",
        "outputId": "fb8aadc0-43e9-4d54-cc6f-47bead2c8846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved dataset22.csv\n",
            "Saved dataset23.csv\n",
            "Saved dataset24.csv\n",
            "Saved dataset25.csv\n",
            "Saved dataset26.csv\n",
            "Saved dataset27.csv\n",
            "Saved dataset28.csv\n",
            "Saved dataset29.csv\n",
            "Saved dataset30.csv\n",
            "Saved dataset31.csv\n",
            "Saved dataset32.csv\n",
            "Saved dataset33.csv\n",
            "Saved dataset34.csv\n",
            "Saved dataset35.csv\n",
            "Saved dataset36.csv\n",
            "Saved dataset37.csv\n",
            "Saved dataset38.csv\n",
            "Saved dataset39.csv\n",
            "Saved dataset40.csv\n",
            "Saved dataset41.csv\n",
            "Saved dataset42.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Label the data.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the new headers for each dataset\n",
        "new_headers1 = [\"pop_size\", \"pop_size_m\", \"mig\", \"mig_m\", \"pct_jobs_ort\", \"pct_jobs_ort_m\", \"place_name\", \"state\", \"place_code\", \"unwanted\", \"year\"]\n",
        "new_headers2 = [\"gini\", \"gini_m\", \"sro_hu\", \"sro_hu_m\", \"hu_tot\", \"hu_tot_m\", \"place_name\", \"state\", \"place_code\", \"unwanted\", \"year\"]\n",
        "new_headers3 = [\"med_earnings\", \"med_earnings_m\", \"pct_bach\", \"pct_bach_m\", \"med_age\", \"med_age_m\", \"place_name\", \"state\", \"place_code\", \"unwanted\", \"year\"]\n",
        "\n",
        "def update_header(file_path, new_headers):\n",
        "    try:\n",
        "        # Read the CSV file into a pandas DataFrame\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if the number of headers match the number of columns\n",
        "        if len(new_headers) != len(df.columns):\n",
        "            print(f\"Error: The number of new headers ({len(new_headers)}) does not match the number of columns in {file_path} ({len(df.columns)})\")\n",
        "            return\n",
        "\n",
        "        # Update the column headers\n",
        "        df.columns = new_headers\n",
        "\n",
        "        # Save the modified DataFrame back to the CSV file\n",
        "        df.to_csv(file_path, index=False)\n",
        "        print(f\"Headers updated for {file_path}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found - {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
        "\n",
        "# Define header groups in a list (repeating sequence)\n",
        "headers = [new_headers1, new_headers2, new_headers3]\n",
        "\n",
        "# Loop through datasets and update headers\n",
        "for i in range(1, 43):  # Dataset numbers from 1 to 42\n",
        "    filename = f\"dataset{i}.csv\"\n",
        "    header = headers[(i - 1) % len(headers)]  # Cycle through headers\n",
        "    update_header(filename, header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lSy0bNL8GKE",
        "outputId": "fbbaa0b1-a9b5-4cb5-a641-8a78239bf395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers updated for dataset1.csv\n",
            "Headers updated for dataset2.csv\n",
            "Headers updated for dataset3.csv\n",
            "Headers updated for dataset4.csv\n",
            "Headers updated for dataset5.csv\n",
            "Headers updated for dataset6.csv\n",
            "Headers updated for dataset7.csv\n",
            "Headers updated for dataset8.csv\n",
            "Headers updated for dataset9.csv\n",
            "Headers updated for dataset10.csv\n",
            "Headers updated for dataset11.csv\n",
            "Headers updated for dataset12.csv\n",
            "Headers updated for dataset13.csv\n",
            "Headers updated for dataset14.csv\n",
            "Headers updated for dataset15.csv\n",
            "Headers updated for dataset16.csv\n",
            "Headers updated for dataset17.csv\n",
            "Headers updated for dataset18.csv\n",
            "Headers updated for dataset19.csv\n",
            "Headers updated for dataset20.csv\n",
            "Headers updated for dataset21.csv\n",
            "Headers updated for dataset22.csv\n",
            "Headers updated for dataset23.csv\n",
            "Headers updated for dataset24.csv\n",
            "Headers updated for dataset25.csv\n",
            "Headers updated for dataset26.csv\n",
            "Headers updated for dataset27.csv\n",
            "Headers updated for dataset28.csv\n",
            "Headers updated for dataset29.csv\n",
            "Headers updated for dataset30.csv\n",
            "Headers updated for dataset31.csv\n",
            "Headers updated for dataset32.csv\n",
            "Headers updated for dataset33.csv\n",
            "Headers updated for dataset34.csv\n",
            "Headers updated for dataset35.csv\n",
            "Headers updated for dataset36.csv\n",
            "Headers updated for dataset37.csv\n",
            "Headers updated for dataset38.csv\n",
            "Headers updated for dataset39.csv\n",
            "Headers updated for dataset40.csv\n",
            "Headers updated for dataset41.csv\n",
            "Headers updated for dataset42.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Delete blank columns.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def delete_second_to_last_column(file_path):\n",
        "    try:\n",
        "        # Read the dataset\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Identify and drop the second-to-last column\n",
        "        if len(df.columns) > 1:  # Ensure there are at least two columns\n",
        "            df = df.drop(df.columns[-2], axis=1)  # Drop the second-to-last column\n",
        "\n",
        "        # Save the modified dataset\n",
        "        df.to_csv(file_path, index=False)\n",
        "        print(f\"Second-to-last column deleted from {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found - {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
        "\n",
        "# Loop through dataset numbers and delete the second-to-last column for each\n",
        "for i in range(1, 43):  # Dataset numbers from 1 to 42\n",
        "    delete_second_to_last_column(f\"dataset{i}.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQkmspp4_x-w",
        "outputId": "73735d87-4e66-42e9-b0d4-8694bcfd8e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second-to-last column deleted from dataset1.csv\n",
            "Second-to-last column deleted from dataset2.csv\n",
            "Second-to-last column deleted from dataset3.csv\n",
            "Second-to-last column deleted from dataset4.csv\n",
            "Second-to-last column deleted from dataset5.csv\n",
            "Second-to-last column deleted from dataset6.csv\n",
            "Second-to-last column deleted from dataset7.csv\n",
            "Second-to-last column deleted from dataset8.csv\n",
            "Second-to-last column deleted from dataset9.csv\n",
            "Second-to-last column deleted from dataset10.csv\n",
            "Second-to-last column deleted from dataset11.csv\n",
            "Second-to-last column deleted from dataset12.csv\n",
            "Second-to-last column deleted from dataset13.csv\n",
            "Second-to-last column deleted from dataset14.csv\n",
            "Second-to-last column deleted from dataset15.csv\n",
            "Second-to-last column deleted from dataset16.csv\n",
            "Second-to-last column deleted from dataset17.csv\n",
            "Second-to-last column deleted from dataset18.csv\n",
            "Second-to-last column deleted from dataset19.csv\n",
            "Second-to-last column deleted from dataset20.csv\n",
            "Second-to-last column deleted from dataset21.csv\n",
            "Second-to-last column deleted from dataset22.csv\n",
            "Second-to-last column deleted from dataset23.csv\n",
            "Second-to-last column deleted from dataset24.csv\n",
            "Second-to-last column deleted from dataset25.csv\n",
            "Second-to-last column deleted from dataset26.csv\n",
            "Second-to-last column deleted from dataset27.csv\n",
            "Second-to-last column deleted from dataset28.csv\n",
            "Second-to-last column deleted from dataset29.csv\n",
            "Second-to-last column deleted from dataset30.csv\n",
            "Second-to-last column deleted from dataset31.csv\n",
            "Second-to-last column deleted from dataset32.csv\n",
            "Second-to-last column deleted from dataset33.csv\n",
            "Second-to-last column deleted from dataset34.csv\n",
            "Second-to-last column deleted from dataset35.csv\n",
            "Second-to-last column deleted from dataset36.csv\n",
            "Second-to-last column deleted from dataset37.csv\n",
            "Second-to-last column deleted from dataset38.csv\n",
            "Second-to-last column deleted from dataset39.csv\n",
            "Second-to-last column deleted from dataset40.csv\n",
            "Second-to-last column deleted from dataset41.csv\n",
            "Second-to-last column deleted from dataset42.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Delete extra characters (1).\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def remove_last_char_place_code(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if 'place_code' in df.columns:\n",
        "            df['place_code'] = df['place_code'].astype(str).str[:-1]\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"Last character of 'place_code' removed in {file_path}\")\n",
        "        else:\n",
        "            print(f\"'place_code' column not found in {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found - {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
        "\n",
        "# Loop through dataset numbers and delete the last column for each\n",
        "for i in range(1, 43):  # Dataset numbers from 1 to 42\n",
        "    remove_last_char_place_code(f\"dataset{i}.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wamsSa6IAKOl",
        "outputId": "6154097f-f9e3-44a5-ee47-d076a7f30a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last character of 'place_code' removed in dataset1.csv\n",
            "Last character of 'place_code' removed in dataset2.csv\n",
            "Last character of 'place_code' removed in dataset3.csv\n",
            "Last character of 'place_code' removed in dataset4.csv\n",
            "Last character of 'place_code' removed in dataset5.csv\n",
            "Last character of 'place_code' removed in dataset6.csv\n",
            "Last character of 'place_code' removed in dataset7.csv\n",
            "Last character of 'place_code' removed in dataset8.csv\n",
            "Last character of 'place_code' removed in dataset9.csv\n",
            "Last character of 'place_code' removed in dataset10.csv\n",
            "Last character of 'place_code' removed in dataset11.csv\n",
            "Last character of 'place_code' removed in dataset12.csv\n",
            "Last character of 'place_code' removed in dataset13.csv\n",
            "Last character of 'place_code' removed in dataset14.csv\n",
            "Last character of 'place_code' removed in dataset15.csv\n",
            "Last character of 'place_code' removed in dataset16.csv\n",
            "Last character of 'place_code' removed in dataset17.csv\n",
            "Last character of 'place_code' removed in dataset18.csv\n",
            "Last character of 'place_code' removed in dataset19.csv\n",
            "Last character of 'place_code' removed in dataset20.csv\n",
            "Last character of 'place_code' removed in dataset21.csv\n",
            "Last character of 'place_code' removed in dataset22.csv\n",
            "Last character of 'place_code' removed in dataset23.csv\n",
            "Last character of 'place_code' removed in dataset24.csv\n",
            "Last character of 'place_code' removed in dataset25.csv\n",
            "Last character of 'place_code' removed in dataset26.csv\n",
            "Last character of 'place_code' removed in dataset27.csv\n",
            "Last character of 'place_code' removed in dataset28.csv\n",
            "Last character of 'place_code' removed in dataset29.csv\n",
            "Last character of 'place_code' removed in dataset30.csv\n",
            "Last character of 'place_code' removed in dataset31.csv\n",
            "Last character of 'place_code' removed in dataset32.csv\n",
            "Last character of 'place_code' removed in dataset33.csv\n",
            "Last character of 'place_code' removed in dataset34.csv\n",
            "Last character of 'place_code' removed in dataset35.csv\n",
            "Last character of 'place_code' removed in dataset36.csv\n",
            "Last character of 'place_code' removed in dataset37.csv\n",
            "Last character of 'place_code' removed in dataset38.csv\n",
            "Last character of 'place_code' removed in dataset39.csv\n",
            "Last character of 'place_code' removed in dataset40.csv\n",
            "Last character of 'place_code' removed in dataset41.csv\n",
            "Last character of 'place_code' removed in dataset42.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Delete extra characters (2).\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def modify_csv(file_path, column_name, chars_to_remove):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if column_name in df.columns:\n",
        "            df[column_name] = df[column_name].astype(str).str[chars_to_remove:]\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"First {chars_to_remove} characters of '{column_name}' removed in {file_path}\")\n",
        "        else:\n",
        "            print(f\"'{column_name}' column not found in {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found - {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
        "\n",
        "# Define the dataset numbers to modify\n",
        "dataset_numbers = range(1, 43, 3)  # Every 3rd dataset starting from 1\n",
        "\n",
        "# Loop through the dataset numbers and call modify_csv\n",
        "for num in dataset_numbers:\n",
        "    modify_csv(f\"dataset{num}.csv\", \"pop_size\", 2)\n",
        "\n",
        "# Define dataset numbers with the desired step\n",
        "dataset_numbers = range(2, 43, 3)  # Datasets 2, 5, 8, ..., 41\n",
        "\n",
        "# Loop through dataset numbers and call modify_csv\n",
        "for num in dataset_numbers:\n",
        "    modify_csv(f\"dataset{num}.csv\", \"gini\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxktQlW8Bkhl",
        "outputId": "c4a4202a-f660-4ccb-bf0d-a1e65db66b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 2 characters of 'pop_size' removed in dataset1.csv\n",
            "First 2 characters of 'pop_size' removed in dataset4.csv\n",
            "First 2 characters of 'pop_size' removed in dataset7.csv\n",
            "First 2 characters of 'pop_size' removed in dataset10.csv\n",
            "First 2 characters of 'pop_size' removed in dataset13.csv\n",
            "First 2 characters of 'pop_size' removed in dataset16.csv\n",
            "First 2 characters of 'pop_size' removed in dataset19.csv\n",
            "First 2 characters of 'pop_size' removed in dataset22.csv\n",
            "First 2 characters of 'pop_size' removed in dataset25.csv\n",
            "First 2 characters of 'pop_size' removed in dataset28.csv\n",
            "First 2 characters of 'pop_size' removed in dataset31.csv\n",
            "First 2 characters of 'pop_size' removed in dataset34.csv\n",
            "First 2 characters of 'pop_size' removed in dataset37.csv\n",
            "First 2 characters of 'pop_size' removed in dataset40.csv\n",
            "First 2 characters of 'gini' removed in dataset2.csv\n",
            "First 2 characters of 'gini' removed in dataset5.csv\n",
            "First 2 characters of 'gini' removed in dataset8.csv\n",
            "First 2 characters of 'gini' removed in dataset11.csv\n",
            "First 2 characters of 'gini' removed in dataset14.csv\n",
            "First 2 characters of 'gini' removed in dataset17.csv\n",
            "First 2 characters of 'gini' removed in dataset20.csv\n",
            "First 2 characters of 'gini' removed in dataset23.csv\n",
            "First 2 characters of 'gini' removed in dataset26.csv\n",
            "First 2 characters of 'gini' removed in dataset29.csv\n",
            "First 2 characters of 'gini' removed in dataset32.csv\n",
            "First 2 characters of 'gini' removed in dataset35.csv\n",
            "First 2 characters of 'gini' removed in dataset38.csv\n",
            "First 2 characters of 'gini' removed in dataset41.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete extra characters (3).\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def modify_csv(file_path, column_name, chars_to_remove):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if column_name in df.columns:\n",
        "            df[column_name] = df[column_name].astype(str).str[chars_to_remove:]\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"First {chars_to_remove} characters of '{column_name}' removed in {file_path}\")\n",
        "        else:\n",
        "            print(f\"'{column_name}' column not found in {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found - {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
        "\n",
        "# Define the dataset numbers\n",
        "dataset_numbers = range(3, 43, 3)  # Every 3rd dataset starting from 3\n",
        "\n",
        "# Loop through the dataset numbers and call modify_csv\n",
        "for num in dataset_numbers:\n",
        "    modify_csv(f\"dataset{num}.csv\", \"med_earnings\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQvFDtYgCuyH",
        "outputId": "6361ae53-3bee-4c2e-e73a-35ca13a27618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 2 characters of 'med_earnings' removed in dataset3.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset6.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset9.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset12.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset15.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset18.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset21.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset24.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset27.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset30.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset33.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset36.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset39.csv\n",
            "First 2 characters of 'med_earnings' removed in dataset42.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Delete extra characters (4).\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def remove_last_char(file_path, column_name):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if column_name in df.columns:\n",
        "            df[column_name] = df[column_name].astype(str).str[:-1]\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"Last character of '{column_name}' removed in {file_path}\")\n",
        "        else:\n",
        "            print(f\"'{column_name}' column not found in {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found - {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
        "\n",
        "# Define the column names in a repeating pattern\n",
        "columns = [\"pop_size\", \"gini\", \"med_earnings\"]\n",
        "\n",
        "# Loop through dataset numbers and dynamically call remove_last_char\n",
        "for i in range(1, 43):  # Dataset numbers 1 to 42\n",
        "    column = columns[(i - 1) % len(columns)]  # Cycle through column names\n",
        "    remove_last_char(f\"dataset{i}.csv\", column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGzQ-kVtDLRA",
        "outputId": "09b2604a-5a72-4983-d804-ff1f6e28fc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last character of 'pop_size' removed in dataset1.csv\n",
            "Last character of 'gini' removed in dataset2.csv\n",
            "Last character of 'med_earnings' removed in dataset3.csv\n",
            "Last character of 'pop_size' removed in dataset4.csv\n",
            "Last character of 'gini' removed in dataset5.csv\n",
            "Last character of 'med_earnings' removed in dataset6.csv\n",
            "Last character of 'pop_size' removed in dataset7.csv\n",
            "Last character of 'gini' removed in dataset8.csv\n",
            "Last character of 'med_earnings' removed in dataset9.csv\n",
            "Last character of 'pop_size' removed in dataset10.csv\n",
            "Last character of 'gini' removed in dataset11.csv\n",
            "Last character of 'med_earnings' removed in dataset12.csv\n",
            "Last character of 'pop_size' removed in dataset13.csv\n",
            "Last character of 'gini' removed in dataset14.csv\n",
            "Last character of 'med_earnings' removed in dataset15.csv\n",
            "Last character of 'pop_size' removed in dataset16.csv\n",
            "Last character of 'gini' removed in dataset17.csv\n",
            "Last character of 'med_earnings' removed in dataset18.csv\n",
            "Last character of 'pop_size' removed in dataset19.csv\n",
            "Last character of 'gini' removed in dataset20.csv\n",
            "Last character of 'med_earnings' removed in dataset21.csv\n",
            "Last character of 'pop_size' removed in dataset22.csv\n",
            "Last character of 'gini' removed in dataset23.csv\n",
            "Last character of 'med_earnings' removed in dataset24.csv\n",
            "Last character of 'pop_size' removed in dataset25.csv\n",
            "Last character of 'gini' removed in dataset26.csv\n",
            "Last character of 'med_earnings' removed in dataset27.csv\n",
            "Last character of 'pop_size' removed in dataset28.csv\n",
            "Last character of 'gini' removed in dataset29.csv\n",
            "Last character of 'med_earnings' removed in dataset30.csv\n",
            "Last character of 'pop_size' removed in dataset31.csv\n",
            "Last character of 'gini' removed in dataset32.csv\n",
            "Last character of 'med_earnings' removed in dataset33.csv\n",
            "Last character of 'pop_size' removed in dataset34.csv\n",
            "Last character of 'gini' removed in dataset35.csv\n",
            "Last character of 'med_earnings' removed in dataset36.csv\n",
            "Last character of 'pop_size' removed in dataset37.csv\n",
            "Last character of 'gini' removed in dataset38.csv\n",
            "Last character of 'med_earnings' removed in dataset39.csv\n",
            "Last character of 'pop_size' removed in dataset40.csv\n",
            "Last character of 'gini' removed in dataset41.csv\n",
            "Last character of 'med_earnings' removed in dataset42.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Keep just CDPs in the western U.S.\n",
        "\n",
        "# Define the list of states to keep\n",
        "states_to_keep = [2, 4, 6, 8, 16, 30, 32, 35, 41, 49, 53, 56]\n",
        "\n",
        "# Function to filter the data\n",
        "def filter_data(df, states_to_keep):\n",
        "    return df[df['state'].isin(states_to_keep)]\n",
        "\n",
        "# Load datasets dynamically into a dictionary\n",
        "dataset1 = pd.read_csv(\"dataset1.csv\")\n",
        "dataset2 = pd.read_csv(\"dataset2.csv\")\n",
        "dataset3 = pd.read_csv(\"dataset3.csv\")\n",
        "dataset4 = pd.read_csv(\"dataset4.csv\")\n",
        "dataset5 = pd.read_csv(\"dataset5.csv\")\n",
        "dataset6 = pd.read_csv(\"dataset6.csv\")\n",
        "dataset7 = pd.read_csv(\"dataset7.csv\")\n",
        "dataset8 = pd.read_csv(\"dataset8.csv\")\n",
        "dataset9 = pd.read_csv(\"dataset9.csv\")\n",
        "dataset10 = pd.read_csv(\"dataset10.csv\")\n",
        "dataset11 = pd.read_csv(\"dataset11.csv\")\n",
        "dataset12 = pd.read_csv(\"dataset12.csv\")\n",
        "dataset13 = pd.read_csv(\"dataset13.csv\")\n",
        "dataset14 = pd.read_csv(\"dataset14.csv\")\n",
        "dataset15 = pd.read_csv(\"dataset15.csv\")\n",
        "dataset16 = pd.read_csv(\"dataset16.csv\")\n",
        "dataset17 = pd.read_csv(\"dataset17.csv\")\n",
        "dataset18 = pd.read_csv(\"dataset18.csv\")\n",
        "dataset19 = pd.read_csv(\"dataset19.csv\")\n",
        "dataset20 = pd.read_csv(\"dataset20.csv\")\n",
        "dataset21 = pd.read_csv(\"dataset21.csv\")\n",
        "dataset22 = pd.read_csv(\"dataset22.csv\")\n",
        "dataset23 = pd.read_csv(\"dataset23.csv\")\n",
        "dataset24 = pd.read_csv(\"dataset24.csv\")\n",
        "dataset25 = pd.read_csv(\"dataset25.csv\")\n",
        "dataset26 = pd.read_csv(\"dataset26.csv\")\n",
        "dataset27 = pd.read_csv(\"dataset27.csv\")\n",
        "dataset28 = pd.read_csv(\"dataset28.csv\")\n",
        "dataset29 = pd.read_csv(\"dataset29.csv\")\n",
        "dataset30 = pd.read_csv(\"dataset30.csv\")\n",
        "dataset31 = pd.read_csv(\"dataset31.csv\")\n",
        "dataset32 = pd.read_csv(\"dataset32.csv\")\n",
        "dataset33 = pd.read_csv(\"dataset33.csv\")\n",
        "dataset34 = pd.read_csv(\"dataset34.csv\")\n",
        "dataset35 = pd.read_csv(\"dataset35.csv\")\n",
        "dataset36 = pd.read_csv(\"dataset36.csv\")\n",
        "dataset37 = pd.read_csv(\"dataset37.csv\")\n",
        "dataset38 = pd.read_csv(\"dataset38.csv\")\n",
        "dataset39 = pd.read_csv(\"dataset39.csv\")\n",
        "dataset40 = pd.read_csv(\"dataset40.csv\")\n",
        "dataset41 = pd.read_csv(\"dataset41.csv\")\n",
        "dataset42 = pd.read_csv(\"dataset42.csv\")\n",
        "\n",
        "# Store datasets in a list for easier access\n",
        "datasets = [dataset1, dataset2, dataset3, dataset4, dataset5, dataset6, dataset7, dataset8, dataset9, dataset10,\n",
        "            dataset11, dataset12, dataset13, dataset14, dataset15, dataset16, dataset17, dataset18, dataset19, dataset20,\n",
        "            dataset21, dataset22, dataset23, dataset24, dataset25, dataset26, dataset27, dataset28, dataset29, dataset30,\n",
        "            dataset31, dataset32, dataset33, dataset34, dataset35, dataset36, dataset37, dataset38, dataset39, dataset40,\n",
        "            dataset41, dataset42]\n",
        "\n",
        "# Filter each dataset and store results in a new list\n",
        "filtered_datasets = [filter_data(dataset, states_to_keep) for dataset in datasets]\n",
        "\n",
        "# Save each filtered dataset to a CSV file\n",
        "for i, dataset in enumerate(filtered_datasets, start=1):\n",
        "    dataset.to_csv(f\"dataset{i}_filtered.csv\", index=False)"
      ],
      "metadata": {
        "id": "l2iJsr4A75Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create annual datasets.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the range of years\n",
        "years = range(2010, 2024)  # From 2010 to 2023\n",
        "\n",
        "# Initialize an empty dictionary to store yearly datasets\n",
        "yearly_datasets = {year: [] for year in years}\n",
        "\n",
        "# Loop through the 39 datasets and group them by year\n",
        "for dataset_num in range(1, 43):\n",
        "    file_name = f\"dataset{dataset_num}_filtered.csv\"\n",
        "    try:\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv(file_name)\n",
        "\n",
        "        # Ensure the 'year' and 'place_code' columns exist\n",
        "        if 'year' in df.columns and 'place_code' in df.columns:\n",
        "            # Convert 'place_code' to int64\n",
        "            df['place_code'] = pd.to_numeric(df['place_code'], errors='coerce').astype('Int64')\n",
        "\n",
        "            # Get the year from the first row\n",
        "            year = df['year'].iloc[0]\n",
        "            if year in yearly_datasets:\n",
        "                yearly_datasets[year].append(df)\n",
        "            else:\n",
        "                print(f\"Unexpected year {year} in {file_name}. Skipping.\")\n",
        "        else:\n",
        "            print(f\"Required columns missing in {file_name}. Skipping.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_name} not found. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_name}: {e}\")\n",
        "\n",
        "# Merge datasets by year on \"place_code\" and \"state\" and save them\n",
        "for year, datasets in yearly_datasets.items():\n",
        "    if datasets:\n",
        "        # Start with the first dataset and merge the rest\n",
        "        merged_df = datasets[0]\n",
        "        for df in datasets[1:]:\n",
        "            # Drop duplicate columns (e.g., 'year', 'place_name') from subsequent datasets\n",
        "            df = df.drop(columns=[col for col in ['year', 'place_name'] if col in df.columns], errors='ignore')\n",
        "\n",
        "            # Merge datasets on \"place_code\" and \"state\"\n",
        "            merged_df = pd.merge(merged_df, df, on=[\"place_code\", \"state\"], how=\"outer\")\n",
        "\n",
        "        # Ensure a single `year` column exists and is set correctly\n",
        "        merged_df['year'] = year\n",
        "\n",
        "        # Remove unnecessary duplicate columns caused by merging\n",
        "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
        "\n",
        "        # Handle column suffixes (_x, _y) by selecting the first non-null value\n",
        "        for col in merged_df.columns:\n",
        "            if col.endswith(\"_x\") or col.endswith(\"_y\"):\n",
        "                base_col = col[:-2]  # Get the base column name\n",
        "                if base_col in merged_df.columns:\n",
        "                    # Combine columns (_x and _y) into the base column\n",
        "                    merged_df[base_col] = merged_df[base_col].combine_first(merged_df[col])\n",
        "                    merged_df.drop(columns=[col], inplace=True)  # Drop the suffixed column\n",
        "\n",
        "        # Save the merged dataset for the year\n",
        "        output_file = f\"yearly_dataset_{year}.csv\"\n",
        "        merged_df.to_csv(output_file, index=False)\n",
        "        print(f\"Saved merged dataset for year {year} to {output_file}\")\n",
        "    else:\n",
        "        print(f\"No datasets found for year {year}.\")"
      ],
      "metadata": {
        "id": "QBEkpwbBG3tN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027a5e99-c101-491c-f4e7-c85c7ef98777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved merged dataset for year 2010 to yearly_dataset_2010.csv\n",
            "Saved merged dataset for year 2011 to yearly_dataset_2011.csv\n",
            "Saved merged dataset for year 2012 to yearly_dataset_2012.csv\n",
            "Saved merged dataset for year 2013 to yearly_dataset_2013.csv\n",
            "Saved merged dataset for year 2014 to yearly_dataset_2014.csv\n",
            "Saved merged dataset for year 2015 to yearly_dataset_2015.csv\n",
            "Saved merged dataset for year 2016 to yearly_dataset_2016.csv\n",
            "Saved merged dataset for year 2017 to yearly_dataset_2017.csv\n",
            "Saved merged dataset for year 2018 to yearly_dataset_2018.csv\n",
            "Saved merged dataset for year 2019 to yearly_dataset_2019.csv\n",
            "Saved merged dataset for year 2020 to yearly_dataset_2020.csv\n",
            "Saved merged dataset for year 2021 to yearly_dataset_2021.csv\n",
            "Saved merged dataset for year 2022 to yearly_dataset_2022.csv\n",
            "Saved merged dataset for year 2023 to yearly_dataset_2023.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Merge annual datasets.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the range of years\n",
        "years = range(2010, 2024)  # From 2010 to 2023\n",
        "\n",
        "# Initialize an empty list to store yearly DataFrames\n",
        "yearly_dataframes = []\n",
        "\n",
        "# Loop through the yearly datasets and load them\n",
        "for year in years:\n",
        "    file_name = f\"yearly_dataset_{year}.csv\"\n",
        "    try:\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv(file_name)\n",
        "\n",
        "        # Append the DataFrame to the list\n",
        "        yearly_dataframes.append(df)\n",
        "        print(f\"Loaded dataset for year {year}.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File {file_name} not found. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {file_name}: {e}\")\n",
        "\n",
        "# Concatenate all yearly datasets into a single DataFrame\n",
        "if yearly_dataframes:\n",
        "    combined_df = pd.concat(yearly_dataframes, ignore_index=True)\n",
        "\n",
        "    # Save the combined dataset to a new CSV file\n",
        "    output_file = \"/[YOUR LOCAL FOLDER LOCATION HERE]/combined_dataset_2010_2023.csv\"\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "    print(f\"Saved combined dataset to {output_file}.\")\n",
        "else:\n",
        "    print(\"No datasets were loaded. Combined dataset was not created.\")"
      ],
      "metadata": {
        "id": "AG_w0nYpIYKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ae376b-0ca7-472c-9089-7d76cf179120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset for year 2010.\n",
            "Loaded dataset for year 2011.\n",
            "Loaded dataset for year 2012.\n",
            "Loaded dataset for year 2013.\n",
            "Loaded dataset for year 2014.\n",
            "Loaded dataset for year 2015.\n",
            "Loaded dataset for year 2016.\n",
            "Loaded dataset for year 2017.\n",
            "Loaded dataset for year 2018.\n",
            "Loaded dataset for year 2019.\n",
            "Loaded dataset for year 2020.\n",
            "Loaded dataset for year 2021.\n",
            "Loaded dataset for year 2022.\n",
            "Loaded dataset for year 2023.\n",
            "Saved combined dataset to /content/drive/MyDrive/Work/IORT/GNAR_Initiative/Research/GNAR_Student_Research/Capstones_Thesis/Elizabeth_Depew/Smith_Reanalysis/combined_dataset_2010_2023.csv.\n"
          ]
        }
      ]
    }
  ]
}